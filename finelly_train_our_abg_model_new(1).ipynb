{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "rwxqsWAJwY-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   ליצור דתא סט של csv \n",
        "שבתוחו יש את המידע שלי\n",
        "2.   להמיר את המידע ל tourch dataset\n",
        "3. לראות אני דוחף את זה למודל והתוצאות יוצאות הגיוניות\n",
        "4. לדחוף אם דתא לואדר ולראות שזה יוצא הגיוני\n",
        "5. אחרי כל זה לנסות לאמן מודל קטן ולהגיע לאובר פיט, להכין הכל ולראות שהוא נשמר והכל\n",
        "6. לאחר מכאן לעשות אימון מלא - לאפטם לאימון את הצורההה \n",
        "7. להוסיך gradient aculimilation\n",
        "8. להוסיף amp\n",
        "9. להוסיף multi gpu\n",
        "10. בכלל לאמן על gpu \n",
        "\n"
      ],
      "metadata": {
        "id": "acYFIZMm240_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY6GIegqwNoL"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "# from datasets import load_dataset\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "\n",
        "class LMDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, model_name_or_path, train_dataset, validation_dataset,#train_path,test_path, line_by_line, pad_to_max_length,\n",
        "                 preprocessing_num_workers, overwrite_cache, max_seq_length, mlm_probability,\n",
        "                 train_batch_size, val_batch_size, dataloader_num_workers):\n",
        "        super().__init__()\n",
        "        self.train_dataset = train_dataset\n",
        "        self.validation_dataset = validation_dataset\n",
        "       # self.line_by_line = line_by_line\n",
        "      #  self.pad_to_max_length = pad_to_max_length\n",
        "      #  self.preprocessing_num_workers = preprocessing_num_workers\n",
        "      #  self.overwrite_cache = overwrite_cache\n",
        "      #  self.max_seq_length = max_seq_length\n",
        "        self.mlm_probability = mlm_probability\n",
        "        self.train_batch_size = train_batch_size\n",
        "        self.val_batch_size = val_batch_size\n",
        "        self.dataloader_num_workers = dataloader_num_workers\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path)\n",
        "        self.data_collator = DataCollatorForLanguageModeling(\n",
        "            tokenizer=tokenizer, mlm_probability=self.mlm_probability)\n",
        "    \"\"\"\n",
        "    def setup(self, stage):\n",
        "        tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path)\n",
        "        extension = self.train_file.split(\".\")[-1]\n",
        "        if extension in (\"txt\", \"raw\"):\n",
        "            extension = \"text\"\n",
        "\n",
        "        data_files = {}\n",
        "        data_files[\"train\"] = self.train_file\n",
        "        data_files[\"validation\"] = self.validation_file\n",
        "        datasets = load_dataset(extension, data_files=data_files)\n",
        "\n",
        "        column_names = datasets[\"train\"].column_names\n",
        "        text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
        "\n",
        "        if self.line_by_line:\n",
        "            # When using line_by_line, we just tokenize each nonempty line.\n",
        "            padding = \"max_length\" if self.pad_to_max_length else False\n",
        "\n",
        "            def tokenize_function(examples):\n",
        "                # Remove empty lines\n",
        "                examples[\"text\"] = [line for line in examples[\"text\"]\n",
        "                                    if len(line) > 0 and not line.isspace()]\n",
        "                return tokenizer(\n",
        "                    examples[\"text\"],\n",
        "                    padding=padding,\n",
        "                    truncation=True,\n",
        "                    max_length=self.max_seq_length,\n",
        "                    # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n",
        "                    # receives the `special_tokens_mask`.\n",
        "                    return_special_tokens_mask=True,\n",
        "                )\n",
        "\n",
        "            tokenized_datasets = datasets.map(\n",
        "                tokenize_function,\n",
        "                batched=True,\n",
        "                num_proc=self.preprocessing_num_workers,\n",
        "                remove_columns=[text_column_name],\n",
        "                load_from_cache_file=not self.overwrite_cache,\n",
        "            )\n",
        "        else:\n",
        "            # Otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.\n",
        "            # We use `return_special_tokens_mask=True` because DataCollatorForLanguageModeling (see below) is more\n",
        "            # efficient when it receives the `special_tokens_mask`.\n",
        "            def tokenize_function(examples):\n",
        "                return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\n",
        "\n",
        "            tokenized_datasets = datasets.map(\n",
        "                tokenize_function,\n",
        "                batched=True,\n",
        "                num_proc=self.preprocessing_num_workers,\n",
        "                remove_columns=column_names,\n",
        "                load_from_cache_file=not self.overwrite_cache,\n",
        "            )\n",
        "\n",
        "            if self.max_seq_length is None:\n",
        "                self.max_seq_length = tokenizer.model_max_length\n",
        "            else:\n",
        "                if self.max_seq_length > tok/enizer.model_max_length:\n",
        "                    warnings.warn(\n",
        "                        f\"The max_seq_length passed ({max_seq_length}) is larger than the maximum length for the\"\n",
        "                        f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
        "                    )\n",
        "                self.max_seq_length = min(self.max_seq_length, tokenizer.model_max_length)\n",
        "\n",
        "            # Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
        "            # max_seq_length.\n",
        "            def group_texts(examples):\n",
        "                # Concatenate all texts.\n",
        "                concatenated_examples = {\n",
        "                    k: sum(examples[k], []) for k in examples.keys()}\n",
        "                total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "                # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "                # customize this part to your needs.\n",
        "                total_length = (total_length // self.max_seq_length) * self.max_seq_length\n",
        "                # Split by chunks of max_len.\n",
        "                result = {\n",
        "                    k: [t[i: i + self.max_seq_length]\n",
        "                        for i in range(0, total_length, self.max_seq_length)]\n",
        "                    for k, t in concatenated_examples.items()\n",
        "                }\n",
        "                return result\n",
        "\n",
        "            # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n",
        "            # remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n",
        "            # might be slower to preprocess.\n",
        "            #\n",
        "            # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
        "            # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
        "            tokenized_datasets = tokenized_datasets.map(\n",
        "                group_texts,\n",
        "                batched=True,\n",
        "                num_proc=self.preprocessing_num_workers,\n",
        "                load_from_cache_file=not self.overwrite_cache,\n",
        "            )\n",
        "\n",
        "        data_collator = DataCollatorForLanguageModeling(\n",
        "            tokenizer=tokenizer, mlm_probability=self.mlm_probability)\n",
        "\n",
        "        train_dataset = tokenized_datasets[\"train\"]\n",
        "        eval_dataset = tokenized_datasets[\"validation\"]\n",
        "        self.train_dataset = train_dataset\n",
        "        self.eval_dataset = eval_dataset\n",
        "        self.data_collator = data_collator  \n",
        "    \"\"\"\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.train_batch_size,\n",
        "            collate_fn=self.data_collator,\n",
        "            num_workers=self.dataloader_num_workers,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.eval_dataset,\n",
        "            batch_size=self.val_batch_size,\n",
        "            collate_fn=self.data_collator,\n",
        "            num_workers=self.dataloader_num_workers,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "לשנות את זה שזה היה קלאס"
      ],
      "metadata": {
        "id": "6xAxspPPz20L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import ArgumentParser\n",
        "import pytorch_lightning as pl\n",
        "from transformers import (\n",
        "    AutoModelForMaskedLM,\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        ")\n",
        "from transformers.optimization import AdamW\n",
        "from data import LMDataModule\n",
        "\n",
        "\n",
        "class LMModel(pl.LightningModule):\n",
        "    def __init__(self, model_name_or_path, learning_rate, adam_beta1, adam_beta2, adam_epsilon):\n",
        "        super().__init__()\n",
        "\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        config = AutoConfig.from_pretrained(\n",
        "            model_name_or_path, return_dict=True)\n",
        "        self.model = AutoModelForMaskedLM.from_pretrained(\n",
        "            model_name_or_path,\n",
        "            config=config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x).logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self.model(**batch).loss\n",
        "        self.log('train_loss', loss, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss = self.model(**batch).loss\n",
        "        self.log('valid_loss', loss, on_step=True, sync_dist=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = AdamW(self.parameters(),\n",
        "                          self.hparams.learning_rate,\n",
        "                          betas=(self.hparams.adam_beta1,\n",
        "                                 self.hparams.adam_beta2),\n",
        "                          eps=self.hparams.adam_epsilon,)\n",
        "        return optimizer\n",
        "\n",
        "    @staticmethod\n",
        "    def add_model_specific_args(parent_parser):\n",
        "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
        "        parser.add_argument('--learning_rate', type=float, default=5e-5)\n",
        "        parser.add_argument('--adam_beta1', type=float, default=0.9)\n",
        "        parser.add_argument('--adam_beta2', type=float, default=0.999)\n",
        "        parser.add_argument('--adam_epsilon', type=float, default=1e-8)\n",
        "        return parser\n",
        "\n",
        "\n",
        "def cli_main():\n",
        "    pl.seed_everything(1234)\n",
        "\n",
        "    # ------------\n",
        "    # args\n",
        "    # ------------\n",
        "    parser = ArgumentParser()\n",
        "    parser.add_argument('--model_name_or_path', type=str,\n",
        "                        default=\"distilbert-base-cased\")\n",
        "    parser.add_argument('--train_file', type=str,\n",
        "                        default=\"data/wikitext-2/wiki.train.small.raw\")\n",
        "    parser.add_argument('--validation_file', type=str,\n",
        "                        default=\"data/wikitext-2/wiki.valid.small.raw\")\n",
        "    parser.add_argument('--line_by_line', action='store_true', default=False)\n",
        "    parser.add_argument('--pad_to_max_length', action='store_true', default=False)\n",
        "    parser.add_argument('--preprocessing_num_workers', type=int, default=4)\n",
        "    parser.add_argument('--overwrite_cache', action='store_true', default=False)\n",
        "    parser.add_argument('--max_seq_length', type=int, default=32)\n",
        "    parser.add_argument('--mlm_probability', type=float, default=0.15)\n",
        "    parser.add_argument('--train_batch_size', type=int, default=4)\n",
        "    parser.add_argument('--val_batch_size', type=int, default=8)\n",
        "    parser.add_argument('--dataloader_num_workers', type=int, default=4)\n",
        "    parser = pl.Trainer.add_argparse_args(parser)\n",
        "    parser = LMModel.add_model_specific_args(parser)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # ------------\n",
        "    # data\n",
        "    # ------------\n",
        "    data_module = LMDataModule(\n",
        "        model_name_or_path=args.model_name_or_path,\n",
        "        train_file=args.train_file,\n",
        "        validation_file=args.validation_file,\n",
        "        line_by_line=args.line_by_line,\n",
        "        pad_to_max_length=args.pad_to_max_length,\n",
        "        preprocessing_num_workers=args.preprocessing_num_workers,\n",
        "        overwrite_cache=args.overwrite_cache,\n",
        "        max_seq_length=args.max_seq_length,\n",
        "        mlm_probability=args.mlm_probability,\n",
        "        train_batch_size=args.train_batch_size,\n",
        "        val_batch_size=args.val_batch_size,\n",
        "        dataloader_num_workers=args.dataloader_num_workers,\n",
        "    )\n",
        "\n",
        "    # ------------\n",
        "    # model\n",
        "    # ------------\n",
        "    lmmodel = LMModel(\n",
        "        model_name_or_path=args.model_name_or_path,\n",
        "        learning_rate=args.learning_rate,\n",
        "        adam_beta1=args.adam_beta1,\n",
        "        adam_beta2=args.adam_beta2,\n",
        "        adam_epsilon=args.adam_epsilon,\n",
        "    )\n",
        "\n",
        "    # ------------\n",
        "    # training\n",
        "    # ------------\n",
        "    trainer = pl.Trainer.from_argparse_args(args)\n",
        "    trainer.fit(lmmodel, data_module)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    cli_main()"
      ],
      "metadata": {
        "id": "5vr_vYSJznUn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}